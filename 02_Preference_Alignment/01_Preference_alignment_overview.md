# üìå Preference Alignment 

## üéØ Obiettivo del Modulo

Apprendere tecniche per **allineare i modelli linguistici (LLM)** alle **preferenze umane**, in modo che le risposte non siano solo corrette, ma anche **coerenti con valori e aspettative umane**.

> üîë **Differenza chiave**:
>
> * **SFT (Supervised Fine-Tuning)** ‚Üí insegna compiti specifici
> * **Preference Alignment** ‚Üí affina il comportamento del modello secondo preferenze umane

---

## üß≠ Panoramica dei Metodi di Allineamento

### üîÅ Pipeline Classica di Allineamento

1. **Supervised Fine-Tuning (SFT)**
   Adattamento iniziale a compiti o domini specifici.
2. **Preference Alignment**
   Ulteriore affinamento per migliorare la qualit√† delle risposte tramite tecniche come:

   * **RLHF** (Reinforcement Learning from Human Feedback)
   * **DPO** (Direct Preference Optimization)
   * **ORPO** (Odds Ratio Preference Optimization)

> ‚ú® Alcuni approcci moderni, come **ORPO**, **fondono** i due stadi in un unico processo semplificato ed efficiente.

---

## 1Ô∏è‚É£ Direct Preference Optimization (DPO)

### ‚úÖ Descrizione

* Approccio **semplificato** all‚Äôallineamento basato su preferenze umane.
* Utilizza direttamente dati di **preferenze** (es: A preferito a B) per **ottimizzare** il modello.
* Evita la necessit√† di:

  * Modelli di reward
  * Algoritmi di reinforcement learning complessi
* √à pi√π **stabile**, pi√π **facile da implementare** e **pi√π efficiente** di RLHF.

### üìö Risorsa di approfondimento

* [Documentazione DPO (Hugging Face)](https://huggingface.co/blog/dpo)

---

## 2Ô∏è‚É£ Odds Ratio Preference Optimization (ORPO)

### ‚úÖ Descrizione

* Tecnica **unificata** che combina:

  * **Instruction Tuning**
  * **Preference Alignment**
* Obiettivo: modificare la funzione di loss della language modeling combinando:

  * **Negative Log-Likelihood Loss**
  * **Termine Odds Ratio (token-level)**

### ‚öôÔ∏è Caratteristiche

* **Single-stage training** (un solo processo addestrativo)
* **Senza modello di riferimento (reference-free)**
* **Efficienza computazionale** superiore
* Ottimi risultati su benchmark come **AlpacaEval**

### üìö Risorsa di approfondimento

* [Documentazione ORPO (Hugging Face)](https://huggingface.co/blog/orpo)

---

## üß™ Esercizi Pratici (con Notebook)

| Titolo                                 | Descrizione Attivit√†                                       | Link                                                                                                            | Colab |
| -------------------------------------- | ---------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ----- |
| **DPO Training**                       | üîπ Usa il dataset HH-RLHF di Anthropic                     |                                                                                                                 |       |
| üî∏ Usa un tuo dataset di preferenze    |                                                            |                                                                                                                 |       |
| üî∏ Prova modelli di dimensioni diverse | [Notebook DPO](https://huggingface.co/blog/dpo#training)   | [Apri in Colab](https://colab.research.google.com/github/huggingface/trl/blob/main/examples/dpo_trainer.ipynb)  |       |
| **ORPO Training**                      | üîπ Allena un modello con dati di istruzione + preferenze   |                                                                                                                 |       |
| üî∏ Varia il peso della loss            |                                                            |                                                                                                                 |       |
| üî∏ Confronta ORPO con DPO              | [Notebook ORPO](https://huggingface.co/blog/orpo#training) | [Apri in Colab](https://colab.research.google.com/github/huggingface/trl/blob/main/examples/orpo_trainer.ipynb) |       |

---

## üìö Altre Risorse

* üîó [Blog di Argilla](https://argilla.io/blog) ‚Äì panoramica su tecniche di alignment
* üîó [Hugging Face TRL Library](https://github.com/huggingface/trl) ‚Äì implementazioni open-source di DPO e ORPO

---

## ‚úÖ Riepilogo

| Metodo                                         | Vantaggi                                            | Limitazioni |
| ---------------------------------------------- | --------------------------------------------------- | ----------- |
| **DPO**                                        | ‚ûï Pi√π semplice di RLHF                              |             |
| ‚ûï Niente reward model                          |                                                     |             |
| ‚ûï Addestramento diretto su preferenze          | ‚ûñ Richiede preferenze annotate                      |             |
| ‚ûñ Meno flessibile di RLHF in alcuni scenari    |                                                     |             |
| **ORPO**                                       | ‚ûï Addestramento unificato (istruzioni + preferenze) |             |
| ‚ûï Architettura reference-free                  |                                                     |             |
| ‚ûï Alta efficienza                              | ‚ûñ Ancora in fase sperimentale                       |             |
| ‚ûñ Necessit√† di buona configurazione della loss |                                                     |             |


