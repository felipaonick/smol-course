{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff91453",
   "metadata": {},
   "source": [
    "## Chat Templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d0c91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\Desktop\\smol-course\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      "<|im_start|>system\n",
      "You are a helpful coding assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a Python function to sort a list<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a Python function to sort a list\"},\n",
    "]\n",
    "\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False\n",
    ")\n",
    "\n",
    "print(\"Formatted chat:\")\n",
    "print(formatted_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3cf673",
   "metadata": {},
   "source": [
    "### add_generate_prompt() Paramenter\n",
    "\n",
    "Aggiunge tokens (ad esempio <assistant>) che indica l'inizio della risposta del modello. Questo assicura che il modello generi un risposta invece che continuare con lo user message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0feb0a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\Desktop\\smol-course\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful coding assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a Python function to sort a list<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\", device_map=\"auto\")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a Python function to sort a list\"},\n",
    "]\n",
    "\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True, \n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd9bc1",
   "metadata": {},
   "source": [
    "Ora passiamo tokenized_chat al metodo .generate() per far si che il modello generi la risposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbc3a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful coding assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a Python function to sort a list<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Here's a Python function that sorts a list of numbers using the Bubble Sort algorithm:\n",
      "\n",
      "```python\n",
      "def bubble_sort(nums):\n",
      "    n = len(nums)\n",
      "    for i in range(n):\n",
      "        for j in range(0, n - i - 1):\n",
      "            if nums[j] > nums[j + 1]:\n",
      "                nums[j], nums[j + 1] = nums[j + 1], nums[j]\n",
      "    return nums\n",
      "\n",
      "# Example usage:\n",
      "numbers = [64, 34, 25, 12, 2\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(tokenized_chat, max_new_tokens=128) \n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265888b",
   "metadata": {},
   "source": [
    "### continue_final_message Parameter\n",
    "\n",
    "Controlla de il messaggio finale della chat deve essere continuato o invece iniziare un messaggio nuovo. Questo rimuove il token di fine sequenza in modo che il modello possa continuare il messaggio finale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6fc5b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9693678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A pirate question, matey! (laughs) Well, I'll tell ye, I've got a few tricks up my sleeve. First, I've got a trusty parrot, Polly, who can eat up to 100 pounds of food in one sitting. But I've also got a special recipe for a \"Hulk's Hoot,\" which is a recipe that's been passed down through me. It's a recipe that's been passed down for generations, and it's said to be the most nutritious food you can eat.\n",
      "\n",
      "But, I'll let ye in on a little secret, matey\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(tokenized_chat, max_new_tokens=128) \n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369f6577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history:\n",
      "system: You are a friendly chatbot who always responds in the style of a pirate\n",
      "user: How many helicopters can a human eat in one sitting?\n",
      "assistant: <|im_start|>system\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A pirate question, matey! (laughs) Well, I'll tell ye, I've got a few tricks up my sleeve. First, I've got a trusty parrot, Polly, who can eat up to 100 pounds of food in one sitting. But I've also got a special recipe for a \"Hulk's Hoot,\" which is a recipe that's been passed down through me. It's a recipe that's been passed down for generations, and it's said to be the most nutritious food you can eat.\n",
      "\n",
      "But, I'll let ye in on a little secret, matey\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"assistant\", \"content\": tokenizer.decode(outputs[0])})\n",
    "\n",
    "print(\"Chat history:\")\n",
    "for message in messages:\n",
    "    print(f\"{message['role']}: {message['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39fb5a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final message:\n",
      "<|im_start|>system\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_start|>system\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A pirate question, matey! (laughs) Well, I'll tell ye, I've got a few tricks up my sleeve. First, I've got a trusty parrot, Polly, who can eat up to 100 pounds of food in one sitting. But I've also got a special recipe for a \"Hulk's Hoot,\" which is a recipe that's been passed down through me. It's a recipe that's been passed down for generations, and it's said to be the most nutritious food you can eat.\n",
      "\n",
      "But, I'll let ye in on a little secret, matey. I've also got a special \"Hulk's Hoot\" that's made from a special blend of ingredients that's said to be the most nutritious food you can eat. It's a recipe that's been passed down through me, and it's said to be the most nutritious food you can eat.\n",
      "\n",
      "So, if ye want to eat a whole Hulk's Hoot, ye'll need to be careful, matey. But, I'll tell ye, I've got a special recipe for a \"Hulk's Hoot\" that's said to be the most nutritious food you can eat. It's\n"
     ]
    }
   ],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=False, continue_final_message=True, return_tensors=\"pt\")\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=128)\n",
    "print(\"Final message:\") \n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7d0f5",
   "metadata": {},
   "source": [
    "Gestione di Chat Templates Multipli\n",
    "\n",
    "## 🔁 Chat Templates Multipli\n",
    "\n",
    "Un **modello può avere più template** per diversi casi d’uso:\n",
    "\n",
    "* 💬 Conversazione regolare\n",
    "* 🔧 Tool use (invocazione strumenti)\n",
    "* 📖 RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "### 📦 Struttura del template multiplo\n",
    "\n",
    "Il `chat_template` è un **dizionario**:\n",
    "\n",
    "```python\n",
    "tokenizer.chat_template = {\n",
    "  \"default\": \"...\",\n",
    "  \"tool_use\": \"...\",\n",
    "  \"rag\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "* `apply_chat_template` usa la chiave `\"default\"` per impostazione predefinita.\n",
    "* Se si passa un parametro `tools` e il template `tool_use` è presente, allora questo viene usato automaticamente.\n",
    "* Per usare un template diverso:\n",
    "\n",
    "```python\n",
    "tokenizer.apply_chat_template(..., chat_template=\"rag\")\n",
    "```\n",
    "\n",
    "### ⚠️ Consiglio pratico\n",
    "\n",
    "> Usa **un unico template** con logica condizionale Jinja per gestire i diversi casi.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Template Selection\n",
    "\n",
    "### 📌 Importanza della coerenza\n",
    "\n",
    "* Prestazioni ottimali si ottengono quando si usa lo **stesso formato di template** su cui il modello è stato addestrato.\n",
    "* Anche durante il fine-tuning, **mantenere costanti i token** del prompt migliora la performance.\n",
    "\n",
    "### 🛠 Casi speciali\n",
    "\n",
    "Se stai addestrando da zero o fine-tunando per chat:\n",
    "\n",
    "* Puoi scegliere un template flessibile come **ChatML**\n",
    "* ChatML **non include BOS/EOS token** → imposta `add_special_tokens=True` se necessari\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Esempio ChatML – Ciclo Jinja\n",
    "\n",
    "Template personalizzato:\n",
    "\n",
    "```jinja2\n",
    "{%- for message in messages %}\n",
    "    {{- '<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>' + '\\\\n' }}\n",
    "{%- endfor %}\n",
    "```\n",
    "\n",
    "### 📌 Versione con generation prompt:\n",
    "\n",
    "```python\n",
    "tokenizer.chat_template = '''\n",
    "{% if not add_generation_prompt is defined %}\n",
    "{% set add_generation_prompt = false %}\n",
    "{% endif %}\n",
    "{% for message in messages %}\n",
    "{{'<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>' + '\\\\n'}}\n",
    "{% endfor %}\n",
    "{% if add_generation_prompt %}\n",
    "{{ '<|im_start|>assistant\\\\n' }}\n",
    "{% endif %}\n",
    "'''.strip()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Ruoli standard nei template\n",
    "\n",
    "Usa ruoli coerenti:\n",
    "\n",
    "* `\"system\"` → istruzioni generali\n",
    "* `\"user\"` → input dell’utente\n",
    "* `\"assistant\"` → risposta del modello\n",
    "\n",
    "Esempio completo:\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n",
    "<|im_start|>user\n",
    "How are you?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "I'm doing great!<|im_end|>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f502aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca42d12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom chat template:\n",
      "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(\"Custom chat template:\")\n",
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "028b08c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>\n",
      "You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n",
      "<|im_start|>\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.\",},\n",
    "    {\"role\": \"user\", \"content\": \"How are you?\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84aa33d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm a chatbot designed to help with everyday tasks. I'm not capable of providing information on complex topics like politics, science, or history. I'm here to assist with everyday tasks, but I don't have the ability to provide information on advanced topics like politics, science, or history. I'm designed to be helpful and informative, but I don't have the capability to provide information on advanced topics.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(tokenized_chat, max_new_tokens=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c5ebc4",
   "metadata": {},
   "source": [
    "# Training di un Modello con Chat Template\n",
    "\n",
    "## 🛠 Perché usare i Chat Template nel Training?\n",
    "\n",
    "* Durante l’addestramento di un modello conversazionale, **formattare i dati** con un chat template assicura che i token appresi riflettano la struttura dei messaggi.\n",
    "* **Importante**: `add_generation_prompt=False` durante il training, per evitare token superflui che servirebbero solo in inferenza.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Esempio: Preprocessing con Chat Template\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "\n",
    "chat1 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n",
    "]\n",
    "chat2 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n",
    "dataset = dataset.map(lambda x: {\n",
    "    \"formatted_chat\": tokenizer.apply_chat_template(\n",
    "        x[\"chat\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "})\n",
    "```\n",
    "\n",
    "### 🖨 Output esempio:\n",
    "\n",
    "```\n",
    "<|user|>\n",
    "Which is bigger, the moon or the sun?</s>\n",
    "<|assistant|>\n",
    "The sun.</s>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Cosa fare dopo?\n",
    "\n",
    "* Usa `formatted_chat` come **input testuale** per addestrare un **causal language model (CLM)**.\n",
    "* Segui la normale ricetta di training per i modelli autoregressivi.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Gestione dei Token Speciali\n",
    "\n",
    "### ❌ Problema:\n",
    "\n",
    "Alcuni tokenizer aggiungono token `<bos>`, `<eos>`, ma i chat template **li includono già**.\n",
    "\n",
    "### ✅ Soluzione:\n",
    "\n",
    "Quando usi `apply_chat_template(tokenize=False)`, **disattiva** l’aggiunta di token speciali:\n",
    "\n",
    "```python\n",
    "apply_chat_template(messages, tokenize=False, add_special_tokens=False)\n",
    "```\n",
    "\n",
    "🔒 **Evita la duplicazione di token speciali**, che può peggiorare l’apprendimento.\n",
    "\n",
    "✅ Se invece imposti `tokenize=True`, il problema non si pone: `apply_chat_template` gestisce tutto correttamente.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 Best Practices\n",
    "\n",
    "* ✅ Usa `add_generation_prompt=False` per il training\n",
    "* ✅ Usa `add_special_tokens=False` se `tokenize=False`\n",
    "* ✅ Applica la formattazione prima del tokenizing\n",
    "* ✅ Verifica che il formato del template sia coerente con l’addestramento originario del modello\n",
    "\n",
    "---\n",
    "\n",
    "📘 *Integrare i chat template nel training garantisce coerenza token-prompt e migliora l’efficacia del fine-tuning su modelli conversazionali.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c05afeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 285.84 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chat', 'formatted_chat'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[[{'content': 'Which is bigger, the moon or the sun?', 'role': 'user'}, {'content': 'The sun.', 'role': 'assistant'}], [{'content': 'Which is bigger, a virus or a bacterium?', 'role': 'user'}, {'content': 'A bacterium.', 'role': 'assistant'}]]\n",
      "['<|im_start|>user\\nWhich is bigger, the moon or the sun?<|im_end|>\\n<|im_start|>assistant\\nThe sun.<|im_end|>\\n', '<|im_start|>user\\nWhich is bigger, a virus or a bacterium?<|im_end|>\\n<|im_start|>assistant\\nA bacterium.<|im_end|>\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "\n",
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "\n",
    "chat1 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n",
    "]\n",
    "chat2 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n",
    "dataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\n",
    "print(dataset)\n",
    "print(dataset['chat'])\n",
    "print(dataset['formatted_chat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f7f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
